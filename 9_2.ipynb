{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wcG4w16g98yr"
      },
      "outputs": [],
      "source": [
        "# 순환 신경망\n",
        "\n",
        "# 데이터 불러오기\n",
        "from tensorflow import keras\n",
        "(train_input, train_target), (test_input, test_target)=\\\n",
        "  keras.datasets.imdb.load_data(num_words=300)\n",
        "  # 상위 빈출 단어 300개만 이용\n",
        "# 문자열로 된 리뷰가 숫자로 치환된 채 저장돼 있음 | 2: 상위 빈출 단어 300개에 미포함된 단어는 모두 2로 처리돼 있음"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련-검증 셋 구분\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_input, val_input, train_target, val_target = train_test_split(train_input, train_target)\n",
        "\n",
        "\n",
        "# 모든 리뷰 데이터 동일 규격 통일 필요\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "train_seq = pad_sequences(train_input, maxlen=100)\n",
        "  # 모든 데이터 최대 길이 100단어로 쪼갬 -> 학습시킬 때 통일된 규격의 데이터 넣어야 하기 때문\n",
        "  # maxlen=: 입력 길이보다 적은 단어는 패딩 넣고, 초과 단어는 입력 길이만큼만 자름\n",
        "    # 초과 단어: 뒤부터 100개 단어만 남김 -> 문장에서 중요한 건 뒤에 있다 설계돼서\n",
        "train_seq[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qix5XwraDl5o",
        "outputId": "f3d74310-dff8-4e6a-a3bc-2ae1b4d43864"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2,  56,   5,   2,  18,  68,  38,   2,   2,  63,  26,   2,   8,\n",
              "       156,   2, 262,   2,   2,  37, 115,  79,  68, 194,   2,  10,  10,\n",
              "         2,   4, 206,   4,   2,   9,  38,  38,  33, 118,   5,   4,   2,\n",
              "         2,   9,   2, 207, 110,  53, 147, 267,   2,  33,   4,   2,   2,\n",
              "         5, 148,  71, 184,   2, 267,  10,  10,   2,   2,   8,  72,   2,\n",
              "        14,   2,   7,   2,  33,  32,   2,  48,  12,  71,   4,  64,  22,\n",
              "        11,   2,  25, 131,  62, 181,   8,   2,  12,  18,   4,   2,   7,\n",
              "       129,   2,   2,   2,  81,  24, 106,  14, 155], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_seq = pad_sequences(val_input, maxlen=100)\n",
        "val_seq.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT8UreHpGLGw",
        "outputId": "033b8346-2a0b-4743-a4aa-a4dbf47b23ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6250, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 만들기\n",
        "model = keras.Sequential() # 모델 선언\n",
        "model.add(keras.layers.SimpleRNN(8, input_shape=(100, 300)))\n",
        "  # 8개의 셀 | 100개의 토큰 들어오는데, 이는 300개의 숫자 범위 내에 들어 있단 것\n",
        "  # SimpleRNN 기본값: tanh 함수 사용\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid')) # 뉴런 1개, 0/1로만 구분하면 되니까 시그모이드 사용"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bulPWL1GWDY",
        "outputId": "a807e8ac-7f4b-48b9-9bc6-3d473b07c61f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_oh = keras.utils.to_categorical(train_seq)\n",
        "  # 단어 하나 표현 위해 300개의 칸 더미변수 처럼 만든 것 -> 숫자 의미 없애기 위해 한 것\n",
        "train_oh.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exKN_bX7I5Qp",
        "outputId": "66f32fa0-3916-41a8-d3d4-227001fe5344"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18750, 100, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_oh = keras.utils.to_categorical(val_seq)\n",
        "val_oh.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jXfo5A2KLZZ",
        "outputId": "ba5c85c9-f600-4655-aa47-9fa674dfcc4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6250, 100, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model.compile(optimizer=rmsprop,\n",
        "              loss='binary_crossentropy', # 긍부정 이진 분류하기에 사용\n",
        "              metrics=['accuracy'])\n",
        "es = keras.callbacks.EarlyStopping(patience=3)\n",
        "result = model.fit(train_oh, train_target, epochs=30,\n",
        "                   validation_data=(val_oh, val_target),\n",
        "                   callbacks=es)"
      ],
      "metadata": {
        "id": "MgD9ChLYKddY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 새 모델 만들기\n",
        "model2 = keras.Sequential()\n",
        "model2.add(keras.layers.Embedding(300, 16, input_length=100))\n",
        "# 16개의 실수들의 집합으로 만들겠단 것, 하나의 리뷰당 100개의 길이 가진 단어 총 300개 들어 있단 것 -> 이를 16개 숫자의 집합으로 만들기\n",
        "model2.add(keras.layers.SimpleRNN(8))\n",
        "model2.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 컴파일\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "es = keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "# 학습\n",
        "model2.fit(train_seq, train_target, epochs=100,\n",
        "           validation_data=(val_seq, val_target),\n",
        "           callbacks=[es])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRF_k9rNMpyn",
        "outputId": "05fc0e66-245d-485c-a448-b4dafb48948e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m586/586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.5821 - loss: 0.6656 - val_accuracy: 0.6750 - val_loss: 0.6069\n",
            "Epoch 2/100\n",
            "\u001b[1m586/586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.7084 - loss: 0.5773 - val_accuracy: 0.7286 - val_loss: 0.5478\n",
            "Epoch 3/100\n",
            "\u001b[1m586/586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.7355 - loss: 0.5367 - val_accuracy: 0.7424 - val_loss: 0.5319\n",
            "Epoch 4/100\n",
            "\u001b[1m586/586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.7494 - loss: 0.5257 - val_accuracy: 0.7474 - val_loss: 0.5291\n",
            "Epoch 5/100\n",
            "\u001b[1m586/586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.7568 - loss: 0.5073 - val_accuracy: 0.7395 - val_loss: 0.5282\n",
            "Epoch 6/100\n",
            "\u001b[1m586/586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step - accuracy: 0.7513 - loss: 0.5110 - val_accuracy: 0.7458 - val_loss: 0.5230\n",
            "Epoch 7/100\n",
            "\u001b[1m586/586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.7563 - loss: 0.5110 - val_accuracy: 0.7461 - val_loss: 0.5237\n",
            "Epoch 8/100\n",
            "\u001b[1m586/586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.7608 - loss: 0.5065 - val_accuracy: 0.7208 - val_loss: 0.5474\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b9793a4bf90>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}